# GoK Huduma Satisfaction Predictor

A comprehensive machine learning application that predicts citizen satisfaction with NYC 311 service requests using advanced data science techniques including sentiment analysis, clustering and ensemble modeling.

## Overview

This project analyzes NYC 311 Resolution Satisfaction Survey data to predict citizen satisfaction levels with government service delivery. The system incorporates sophisticated data preprocessing, exploratory data analysis, feature engineering with NLP techniques, machine learning model development, and deployment through both interactive web interfaces and REST APIs.

## Key Features

- **Advanced ML Pipeline**: Custom satisfaction prediction pipeline with Random Forest classifier
- **NLP Integration**: Sentiment analysis and text clustering for feedback processing
- **Interactive Web App**: Streamlit interface with real-time predictions
- **REST API**: FastAPI backend for programmatic access and integration
- **Comprehensive Analytics**: Full data science workflow from EDA to deployment
- **High Performance**: Achieves 98.3% recall, 92.6% F1-score, and 97.1% AUC

## Project Architecture

```
Phase5_Capstone/
├── Core Application Files:
├── app_api.py                           # FastAPI REST API server (main)
├── script_api.py                        # Alternative FastAPI implementation
├── script_streamlit.py                   # Streamlit web application
├── satisfaction_pipeline.py             # Custom ML pipeline class
├── data_processor.py                    # Data loading and cleaning utilities
├── data_analysis.py                     # EDA and visualization classes
├── 
├── Analysis Notebooks:
├── data_preparation.ipynb               # Data cleaning and preprocessing
├── EDA.ipynb                           # Exploratory data analysis
├── modelling.ipynb                     # Model development and evaluation
├── deployment.ipynb                    # Deployment preparation
├── 
├── Configuration:
├── .gitignore                          # Git ignore rules
├── requirements.txt                    # Python dependencies
├── LICENSE                             # MIT License
└── README.md                           # Project documentation
```

**Note**: Data files (CSV) and trained model files (.joblib/.pkl) are not included in the repository and need to be generated by running the notebooks or provided separately.

## Technical Implementation

### Data Processing Pipeline

**1. Data Loading & Cleaning (`data_processor.py`)**
- Handles 364,689 survey responses across 11 features
- Intelligent missing value imputation using agency mappings
- Conditional filling of dissatisfaction reasons based on satisfaction levels
- Placeholder assignment for incomplete data fields

**2. Advanced Feature Engineering**
- **Sentiment Analysis**: VADER sentiment scoring on combined feedback text
- **Text Clustering**: K-means clustering on dissatisfaction reasons using TF-IDF
- **Categorical Encoding**: Label encoding for agency names, complaint types, and boroughs
- **Temporal Features**: Survey year and month for seasonal pattern detection

**3. Exploratory Data Analysis (`data_analysis.py`)**
- Comprehensive statistical analysis across 19 agencies and 206 complaint types
- Satisfaction distribution analysis by borough, agency, and complaint type
- Temporal trend analysis and seasonal pattern identification
- Interactive visualizations using matplotlib and seaborn

### Machine Learning Pipeline

**Custom SatisfactionPipeline Class (`satisfaction_pipeline.py`)**
```python
class SatisfactionPipeline:
    - Binary target creation (Satisfied: Strongly Agree/Agree vs Others)
    - Automated categorical encoding with label encoders
    - Random Forest classifier with 100 estimators
    - Built-in model persistence and loading capabilities
    - Feature importance analysis and performance metrics
```

**Model Performance:**
- **Recall**: 98.3% (excellent at identifying satisfied citizens)
- **F1 Score**: 92.6% (balanced precision and recall)
- **AUC Score**: 97.1% (strong discriminative ability)
- **Top Features**: Cluster (90.7%), Complaint Type (3.5%), Survey Month (1.8%)

**Model Comparison:**
| Metric | Random Forest | XGBoost | Selected |
|--------|---------------|---------|----------|
| F1 Score | 0.926 | 0.932 | ✓ Random Forest |
| Recall | 0.983 | 0.999 | (Better interpretability) |
| AUC Score | 0.971 | 0.963 | |

## Installation & Setup

### Prerequisites
```bash
# Install all dependencies from requirements.txt
pip install -r requirements.txt

# Or install manually:
pip install pandas numpy scikit-learn joblib
pip install streamlit fastapi uvicorn pydantic
pip install matplotlib seaborn nltk textblob wordcloud
pip install xgboost shap yellowbrick requests
```

### Quick Start
```bash
# Clone repository
git clone https://github.com/billysambasi/Phase5_Capstone.git
cd Phase5_Capstone

# Install dependencies
pip install -r requirements.txt

# Generate model file by running the notebooks:
# 1. Run data_preparation.ipynb to process data
# 2. Run EDA.ipynb for analysis
# 3. Run modelling.ipynb to train and save model as 'satisfaction_model.pkl'

# Run FastAPI server (main implementation)
uvicorn app_api:app --reload

# Or run alternative FastAPI server
uvicorn script_api:app --reload

# Run Streamlit app (requires API server running)
streamlit run script_streamlit.py
```

## Application Interfaces

### Streamlit Web Application

**Features:**
- **Single Prediction Mode**: Interactive form for individual complaint analysis
- **Batch Prediction Mode**: CSV file upload for bulk predictions
- Dynamic agency-complaint type mapping
- Real-time API integration with FastAPI backend
- Progress tracking for batch processing
- Downloadable prediction results
- User-friendly interface for non-technical users

**Usage:**
```bash
# Ensure FastAPI server is running first
uvicorn app_api:app --reload

# Then run Streamlit app
streamlit run script_streamlit.py
# Access at http://localhost:8501
```

### FastAPI REST API

**Endpoints:**
- `GET /` - API status and model information
- `GET /health` - Health check endpoint
- `POST /predict` - Satisfaction prediction endpoint
- `POST /reload-model` - Hot model reloading

**Prediction Request Format:**
```json
{
    "agency_name": "Department of Transportation",
    "complaint_type": "Street Condition", 
    "borough": "MANHATTAN",
    "year": 2024,
    "month": 6,
    "cluster": 2,
    "sentiment_score": 0.1
}
```

**Response Format:**
```json
{
    "satisfaction_probability": 0.8542,
    "prediction": 1,
    "prediction_label": "Satisfied",
    "model_loaded_at": "2024-01-15T10:30:00"
}
```

## Data Science Workflow

### 1. Data Understanding & Preparation

**Dataset Characteristics:**
- **Size**: 364,689 survey responses
- **Features**: 11 original features + 4 engineered features
- **Target**: Binary satisfaction (Strongly Agree/Agree vs Others)
- **Agencies**: 19 NYC government agencies
- **Complaint Types**: 206 different service request categories
- **Time Range**: 2022-2025 survey responses

**Data Quality Issues Addressed:**
- Missing agency names (94 records) - filled using acronym mappings
- Missing descriptors (12,975 records) - filled with "Not Provided"
- Missing dissatisfaction reasons (140,077 records) - conditionally filled
- Borough information gaps (438 records) - filled with "Unknown Borough"

### 2. Feature Engineering & NLP

**Text Processing Pipeline:**
```python
# Sentiment Analysis
from textblob import TextBlob
sentiment_scores = df['Combined_Feedback'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Text Clustering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
clusters = KMeans(n_clusters=5).fit_predict(tfidf_matrix)
```

**Engineered Features:**
- **Sentiment Score**: Polarity score (-1 to 1) from combined feedback text
- **Cluster**: K-means clusters (0-4) based on dissatisfaction patterns
- **Combined_Feedback**: Concatenated justified dissatisfaction and reason text
- **Sentiment Label**: Categorical sentiment (positive/neutral/negative)

### 3. Model Development & Evaluation

**Training Process:**
1. Binary target creation from 5-point satisfaction scale
2. Categorical feature encoding using LabelEncoder
3. 80/20 train-test split with stratification
4. Random Forest training with 100 estimators
5. Performance evaluation using multiple metrics

**Feature Importance Analysis:**
```
Cluster (90.7%): Dominant predictor based on dissatisfaction patterns
Complaint Type (3.5%): Service category influence
Survey Month (1.8%): Seasonal satisfaction variations
Sentiment Score (1.5%): Text sentiment impact
Borough (1.0%): Geographic satisfaction differences
```

## Advanced Analytics

### Satisfaction Analysis by Dimensions

**By Agency Performance:**
- NYPD: Highest volume (165,140 requests) with mixed satisfaction
- DOB: Building-related complaints with moderate satisfaction
- DSNY: Sanitation services with seasonal variations

**By Complaint Categories:**
- Illegal Parking: Most common (81,527 requests)
- Heat/Hot Water: High dissatisfaction in winter months
- Noise Complaints: Consistent moderate satisfaction

**By Geographic Distribution:**
- Brooklyn: Highest volume (129,662 requests)
- Manhattan: Higher satisfaction rates
- Staten Island: Lowest volume but higher satisfaction

### Temporal Patterns
- **Seasonal Trends**: Higher dissatisfaction in winter months
- **Year-over-Year**: Gradual improvement in satisfaction rates
- **Monthly Variations**: Peak complaint volumes in summer

## Model Interpretability

**SHAP Analysis Integration:**
```python
import shap
explainer = shap.TreeExplainer(model.pipeline.named_steps['classifier'])
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)
```

**Business Insights:**
1. **Cluster Analysis**: Dissatisfaction patterns are the strongest predictor
2. **Service Type Impact**: Certain complaint types consistently underperform
3. **Temporal Effects**: Satisfaction varies by season and survey timing
4. **Geographic Disparities**: Borough-level satisfaction differences

## Deployment & Production

### Model Persistence
```python
# Save complete pipeline with encoders
model_data = {
    'pipeline': self.pipeline,
    'label_encoders': self.label_encoders,
    'feature_names': self.feature_names
}
joblib.dump(model_data, 'best_model.joblib')
# Also supports satisfaction_model.pkl format
```

### API Integration
- **Hot Reloading**: Update models without server restart
- **Error Handling**: Comprehensive exception management
- **Input Validation**: Pydantic models for request validation
- **Logging**: Request/response logging for monitoring

### Performance Monitoring
- **Model Metrics**: Continuous performance tracking
- **Data Drift**: Feature distribution monitoring
- **Prediction Confidence**: Probability threshold analysis

## Setup Workflow

### Complete Setup Process
1. **Clone Repository**
   ```bash
   git clone https://github.com/billysambasi/Phase5_Capstone.git
   cd Phase5_Capstone
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Obtain Data**
   - Download NYC 311 Resolution Satisfaction Survey data
   - Place CSV files in project root directory

4. **Generate Model**
   - Run `data_preparation.ipynb` for data cleaning
   - Run `EDA.ipynb` for exploratory analysis
   - Run `modelling.ipynb` to train and save model
   - Ensure model is saved as `satisfaction_model.pkl`

5. **Start Services**
   ```bash
   # Terminal 1: Start API server
   uvicorn app_api:app --reload
   
   # Terminal 2: Start Streamlit app
   streamlit run script_streamlit.py
   ```

6. **Access Applications**
   - FastAPI: http://localhost:8000
   - Streamlit: http://localhost:8501

## Development Workflow

### Notebook-Driven Development
1. **data_preparation.ipynb**: Data cleaning and initial preprocessing
2. **EDA.ipynb**: Comprehensive exploratory analysis with visualizations
3. **modelling.ipynb**: Model development, comparison, and evaluation
4. **deployment.ipynb**: Production deployment preparation

### Code Organization
- **Object-Oriented Design**: Modular classes for different functionalities
- **Pipeline Architecture**: Reusable ML pipeline with sklearn compatibility
- **Separation of Concerns**: Clear separation between data, model, and API layers

## Current Implementation Status

### Working Components
- ✅ **Data Processing Pipeline**: Complete data cleaning and preprocessing utilities
- ✅ **ML Pipeline**: Custom SatisfactionPipeline class with Random Forest
- ✅ **Analysis Notebooks**: Complete Jupyter notebooks for EDA and modeling
- ✅ **FastAPI Backend**: Two implementations (app_api.py and script_api.py)
- ✅ **Streamlit Interface**: Full implementation with single and batch prediction modes
- ✅ **Model Persistence**: Support for both .pkl and .joblib formats
- ✅ **Requirements Management**: Complete dependency specification

### Setup Requirements
- ⚠️ **Data Files**: CSV datasets need to be obtained and placed in project directory
- ⚠️ **Model Files**: Trained model files need to be generated by running notebooks
- ⚠️ **API Integration**: Streamlit app requires FastAPI server to be running

## Future Enhancements

### Technical Improvements
- **Deep Learning**: LSTM/BERT models for text analysis
- **Ensemble Methods**: Stacking multiple algorithms
- **Real-time Learning**: Online learning capabilities
- **A/B Testing**: Model comparison framework

### Business Applications
- **Proactive Interventions**: Early warning system for dissatisfaction
- **Resource Allocation**: Optimize agency resource distribution
- **Service Quality Metrics**: KPI dashboard for agencies
- **Citizen Feedback Loop**: Automated response system

## Contributing

### Development Setup
```bash
# Fork and clone repository
git clone https://github.com/billysambasi/Huduma_Satisfaction_Prediction.git
cd Phase5_Capstone

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/
```

### Contribution Guidelines
1. **Code Style**: Follow PEP 8 guidelines
2. **Documentation**: Update docstrings and README
3. **Testing**: Add unit tests for new features
4. **Performance**: Benchmark model changes
5. **Security**: Validate input sanitization

## License & Acknowledgments

**License**: MIT License - see [LICENSE](LICENSE) file

**Data Source**: [NYC Open Data - 311 Resolution Satisfaction Survey](https://catalog.data.gov/dataset/311-resolution-satisfaction-survey-e4003)

**Technologies Used**:
- **ML/DS**: scikit-learn, pandas, numpy, matplotlib, seaborn
- **NLP**: NLTK, TextBlob, TF-IDF vectorization
- **Web**: Streamlit, FastAPI, Uvicorn
- **Deployment**: Joblib model persistence, Pydantic validation

---

*This project demonstrates end-to-end machine learning implementation for government service satisfaction prediction, incorporating modern data science best practices and production-ready deployment strategies.*
